{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello World, begin project\n"
     ]
    }
   ],
   "source": [
    "print('Hello World, begin project')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem statemant:\n",
    "\n",
    "1. Extract the news articles links of last 30 days using python\n",
    "\n",
    "2. From each link extract the Title, Body, date.\n",
    "\n",
    "3. Clean the text body and remove the noise in it\n",
    "\n",
    "4. Create the DB named news and table named articles in the DB (any DB is okay)\n",
    "\n",
    "4. Save the fields in the Database table and Excel sheet\n",
    "***\n",
    "Data Source link: https://www.moneycontrol.com/\n",
    "***"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from time import sleep\n",
    "\n",
    "# Libraries for web Scraping\n",
    "import requests\n",
    "from bs4 import BeautifulSoup  \n",
    "\n",
    "# Libraries for date operations\n",
    "from dateutil import parser  \n",
    "from datetime import timedelta\n",
    "import datetime\n",
    "\n",
    "# Libraries for text cleaning and niose removal\n",
    "import unidecode\n",
    "import re\n",
    "\n",
    "# Libraries for export to database and excel\n",
    "from pymongo import MongoClient\n",
    "from openpyxl import load_workbook\n",
    "import xlsxwriter"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### User-Defined Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_art_list_from_page (url: str) :\n",
    "    '''\n",
    "    Function to extract the links of all news article from a page (each page contains about 25 articles).\n",
    "    \n",
    "    | Input: URL to all news page with page number\n",
    "    | Output: 1) List of article URLs in given page\n",
    "            2) Publication date of the last article in the given page (to check for end of scraping) \n",
    "    '''\n",
    "\n",
    "    art_url_list =[]\n",
    "    response = requests.get(url)\n",
    "\n",
    "    article_list = BeautifulSoup(response.text,'html.parser') # parse the news page\n",
    "    links = article_list.find_all('li',attrs={'class':'clearfix'}) # extract all tags with article links\n",
    "    \n",
    "    for link in links:\n",
    "        art_url_list.append(link.find('a').get(\"href\")) # get link of article \n",
    "        last_article_date =  parser.parse(link.find('span').get_text(),ignoretz=True).date() # get date of published article\n",
    "    \n",
    "    return art_url_list, last_article_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_cleaner(input_text:str):\n",
    "    ''' \n",
    "    Function for noise removal and text cleaning.\n",
    "    Operations: lower casing, remove non-unicode characters, remove links, remove numbers, remove extra spaces. \n",
    "\n",
    "    | Input: raw text\n",
    "    | Output: processed text\n",
    "    '''\n",
    "    \n",
    "    input_text = unidecode.unidecode(input_text) # removes non-breaking white spaces\n",
    "\n",
    "    input_lower = input_text.lower() # lower case all characters\n",
    "\n",
    "    clean1 = re.sub(r\"(@[a-z0-9]+)|([^a-z \\t])|(\\w+:\\/\\/\\S+)|\\bwww\\.[^\\s]+|http.+?\", \"\", input_lower)\n",
    "    # @[A-Za-z0-9] matches @ mentions, email addresses\n",
    "    # [^0-9A-Za-z \\t] matches non alphabet characters\n",
    "    # \\w+:\\/\\/\\S+ matches URLs\n",
    "    # \\bwww\\.[^\\s] matches web addresses starting with www\n",
    "    # http.+? matches URL starting with http\n",
    "\n",
    "    processed_text = re.sub(\"(\\s){2,}\", \" \", clean1) # remove extra spaces\n",
    "\n",
    "    return processed_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_article_page (art_url:str):\n",
    "    ''' \n",
    "    Function to extract and process news article from URL of article. \n",
    "    Extracted parameters: Title of article, Publication date, Text body of article. \n",
    "    Text body is further cleaned using text_cleaner() function.\n",
    "    Returns dictionary which is ready for export and data storage. \n",
    "    Returns empty dictionary if the URL is not a news article or if any error is encountered during parsing.\n",
    "\n",
    "    | Input: URL of article\n",
    "    | Output: Dictionary with following keys: 'URL', 'Title', 'Publish_Date', Content' \n",
    "            Empty dictionary returned if any anomolous cases(mentioned above) are found\n",
    "    '''\n",
    "\n",
    "    try:\n",
    "                \n",
    "        article_full_content = []\n",
    "        get_article = requests.get(art_url)\n",
    "        \n",
    "        article_soup_obj = BeautifulSoup(get_article.content,'html.parser') # parse page \n",
    "        article_title = article_soup_obj.find(\"h1\",attrs={\"class\":\"article_title artTitle\"}).get_text() # get article title\n",
    "        article_date =  article_soup_obj.find(\"div\",attrs={\"class\":\"article_schedule\"}).find('span').get_text() # get article date\n",
    "        article_body =  article_soup_obj.find(\"div\",attrs={\"class\":\"content_wrapper arti-flow\"}) # locate article content\n",
    "        article_paragraphs = article_body.findAll(\"p\") # locate paragraphs in article content\n",
    "        for i in article_paragraphs:\n",
    "            article_full_content.append(i.get_text()) # extract text from paragraph\n",
    "\n",
    "        article_full_content = ' '.join(article_full_content) # combine all paragraph contents\n",
    "        \n",
    "        # call text_cleaner() for processing the text content\n",
    "        article_full_content = text_cleaner(article_full_content) \n",
    "\n",
    "        # create dictionary for export\n",
    "        export_dict = {'URL': art_url, 'Title': article_title, 'Publish_Date': article_date, 'Content': article_full_content}\n",
    "        return export_dict    \n",
    "    except:\n",
    "        return {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_to_db(dict_to_db:dict, DB_name:str, table_name:str):\n",
    "    ''' \n",
    "    Function to export dictionary containing article details to MongoDB. \n",
    "\n",
    "    | Input: dictionary for export, Database name, Table name\n",
    "    '''\n",
    "\n",
    "    client=MongoClient('mongodb://192.168.1.2:27017') # connect to database\n",
    "    db=client[DB_name] # access database, if not exist then create database\n",
    "    collection = db[table_name] # access table/collection, create it not exist\n",
    "\n",
    "    collection.insert_one(dict_to_db) # store dictionary in table/collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_to_excel(dict_to_xlsx:dict, WB_name:str, sheet_name:str):\n",
    "    ''' \n",
    "    Function to export dictionary containing article details to Excel workbook.\n",
    "    Each function call will append data to workbook.\n",
    "\n",
    "    | Input: dictionary for export, Workbook name, Sheet name\n",
    "    '''\n",
    "\n",
    "    if WB_name[-5:] != '.xlsx': # add extension to workbook name\n",
    "        WB_name = WB_name + '.xlsx'\n",
    "\n",
    "    headers = ['URL','Title','Publish_Date','Content'] # Column headers\n",
    "    \n",
    "    if not os.path.isfile(WB_name): # create workbook if it does not exist\n",
    "        book = xlsxwriter.Workbook(WB_name)\n",
    "        sheet = book.add_worksheet(sheet_name)\n",
    "        for (idx, header) in enumerate(headers):\n",
    "            sheet.write(0, idx, header)\n",
    "        book.close()\n",
    "    \n",
    "    loaded_book = load_workbook(WB_name)\n",
    "    loaded_sheet = loaded_book[sheet_name]\n",
    "\n",
    "    values = [dict_to_xlsx[key] for key in headers] # extract values for each key from dictionary \n",
    "    \n",
    "    loaded_sheet.append(values) # write to excel file\n",
    "    loaded_book.save(filename=WB_name)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-06-30 || Page no:  3\r"
     ]
    }
   ],
   "source": [
    "run_start_date = datetime.datetime.now().date() # date at time of program execution\n",
    "page_last_article_date = datetime.datetime.now().date() # variable definition to enter loop, will be updated after entering loop \n",
    "last_date_for_extraction = run_start_date - timedelta(days=0) # last date till when article must be extracted \n",
    "\n",
    "# variable initializations\n",
    "page_no = 1\n",
    "full_list_art = [] \n",
    "total_num_of_articles = 0\n",
    "\n",
    "while page_last_article_date >= last_date_for_extraction: # main loop | checking date for end condition\n",
    "    url = 'https://www.moneycontrol.com/news/news-all/page-' + str(page_no) + '/'  # All News page URL of news website with page number  \n",
    "    \n",
    "    art_url_list, page_last_article_date = get_art_list_from_page(url) # get list of URLs from given page, get publish date of last article in given page\n",
    "\n",
    "    for art_url in art_url_list: # loop to iterare elements in list of article URL \n",
    "\n",
    "        export_dict = get_article_page(art_url) # Scraping article URL for data\n",
    "\n",
    "        if bool(export_dict): # check if empty dictionary\n",
    "            total_num_of_articles += 1\n",
    "            export_to_db(export_dict, DB_name='News', table_name='Articles') # export to MongoDB\n",
    "            export_to_excel(export_dict, WB_name='News', sheet_name='Articles') # export to Excel file \n",
    "            \n",
    "    print(page_last_article_date, '|| Page no: ', page_no, end=\"\\r\")\n",
    "    page_no += 1  # move to next page in article list page \n",
    "\n",
    "print('Total number of articles processed: ', total_num_of_articles)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
